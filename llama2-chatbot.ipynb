{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cd06f1-0c15-46c7-a002-0ee7f9b9e22d",
   "metadata": {},
   "source": [
    "## Les prérequis\n",
    "Installer la librairie llama-cpp, vous pouvez installer directement depuis le fichier wheel qui correspond à votre environnement\n",
    "### Préparer votre environnement\n",
    "- Lien vers le github de llama-cpp : https://github.com/ggerganov/llama.cpp\n",
    "- pip install llama-cpp-python ou llama-cpp-ggml ou directement depuis le github\n",
    "- Si l'installation avec pip n'a pas marché télécharger le fichier binaire pré construit qu'on appelle wheels depuis : https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/basic/cpu/llama-cpp-python-ggml/\n",
    "- Enuiste : pip install \"mon_fichier.whl\"\n",
    "- pip install gradio\n",
    "### Télécharger les poids du modèle\n",
    "Vous devez télécharger la version du model llama 2 en ggml, vous pouvez choisir la version des poids que vous voulez ici : https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29da2dfe-fb93-4bd6-9ff3-5c503be2df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://f8541d16a38d3f7bcb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f8541d16a38d3f7bcb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp_ggml import Llama\n",
    "import gradio as gr\n",
    "\n",
    "name = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "llama = Llama(model_path=name) # modifier les autres paramètres pour adapté le fonctionnement à votre cas d'usage\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "    def respond(message, chat_history):\n",
    "        prompt = \"Answers like a chatbot in french only with the bare minimum ##question:{} ##answer:\".format(message)\n",
    "        bot_message = llama(prompt, max_tokens=20)['choices'][0]['text']\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582ab34-6e80-4dd7-a41c-6f2f056ff401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e719c-22c2-478d-ae17-c1c48468f104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
